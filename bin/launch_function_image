#!/usr/bin/env bash

# launch Lambda function
# for local testing

directory_name=$1

if [[ -z "$directory_name" ]]; then
    echo "valid directory name required:"
    echo ""
    ls -li cmd/lambda | awk '{print $NF}' | tail -n +2
    echo ""
    exit 0
fi

aws_access_key_id=$(aws --profile default configure get aws_access_key_id)
aws_secret_access_key=$(aws --profile default configure get aws_secret_access_key)
aws_default_region=$(aws --profile default configure get region)

s3_data_bucket_name=$(python3 ./bin/helpers/get_parameter.py S3DataBucketName)
alpaca_api_key=$(python3 ./bin/helpers/get_parameter.py AlpacaAPIKey)
alpaca_api_secret=$(python3 ./bin/helpers/get_parameter.py AlpacaAPISecret)
alpha_vantage_api_key=$(python3 ./bin/helpers/get_parameter.py AlphaVantageAPIKey)
darqube_api_key=$(python3 ./bin/helpers/get_parameter.py DarqubeAPIKey)
model_file_path=$(python3 ./bin/helpers/get_parameter.py ModelFilePath)
is_paper=$(python3 ./bin/helpers/get_parameter.py IsPaper)
sns_errors_topic_arn=$(python3 ./bin/helpers/get_parameter.py SNSErrorsTopicARN)

image_name=$(python3 ./bin/helpers/rename_resource.py ${directory_name})

docker run \
    --publish 9000:8080 \
    --env AWS_ACCESS_KEY_ID=${aws_access_key_id} \
    --env AWS_SECRET_ACCESS_KEY=${aws_secret_access_key} \
    --env AWS_DEFAULT_REGION=${aws_default_region} \
    --env S3_DATA_BUCKET_NAME=${s3_data_bucket_name} \
    --env ALPACA_API_KEY=${alpaca_api_key} \
    --env ALPACA_API_SECRET=${alpaca_api_secret} \
    --env ALPHA_VANTAGE_API_KEY=${alpha_vantage_api_key} \
    --env MODEL_FILE_PATH=${model_file_path} \
    --env DARQUBE_API_KEY=${darqube_api_key} \
    --env IS_PAPER=${is_paper} \
    --env SNS_ERRORS_TOPIC_ARN=${sns_errors_topic_arn} \
    ${image_name}:latest
